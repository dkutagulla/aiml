{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Notes",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bMOBpRPHSiarG-Av0_lMIVQNttwj0xQ0",
      "authorship_tag": "ABX9TyP8wq4cMBmbcMRKr92rXhKr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkutagulla/aiml/blob/AI_ML_ForCoders/Machine_Learning_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C1BjYg-3S4v"
      },
      "source": [
        "# Notes and code\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tJ4m7FPJyxl"
      },
      "source": [
        "My notes of ML book I am reading **AI and Machine Learning for Coders**\r\n",
        "\r\n",
        "Notes using TensorFlow API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRsh-K-f3wp7"
      },
      "source": [
        "# TensorFlow version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KakY95S83AVu",
        "outputId": "f03c6132-74dd-431c-e526-faa6767af635"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzjbXtrf5FMu"
      },
      "source": [
        "# Model \r\n",
        "1. relationship that takes input(s) and produces output(s)\r\n",
        "2. contains (set of)  parameters that transform input to output.\r\n",
        "idea is to have a set of parameters that can apply to all input(s) values \r\n",
        "Neurons are the individual building blocks of the models\r\n",
        "\r\n",
        "   \r\n",
        "# Simple example\r\n",
        "Determine relation (model)  between x and y based on input data\r\n",
        "1. build the model using a sample of x input values and y output values and use it to predict a value of y given a x value.\r\n",
        "2. have tf output parameters of the model.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGoFYvgDQ2PC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSWoAj6t4Aqg"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from tensorflow.keras import Sequential\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "\r\n",
        "first_layer = Dense( \r\n",
        "    units=1, # how many neurons \r\n",
        "    input_shape=[1] # how many inputs to neuron ;\r\n",
        "    # ONLY specified for first layer\r\n",
        "    )\r\n",
        "\r\n",
        "# model has array of layers inside the Sequential container \r\n",
        "model = Sequential( \r\n",
        "                   [ # layer description  ; array of layers\r\n",
        "                       first_layer\r\n",
        "                   ]  \r\n",
        "    )\r\n",
        "\r\n",
        "# setup training method sgd = stochastic gradient descent\r\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\r\n",
        "\r\n",
        "# example data to develop model\r\n",
        "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\r\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\r\n",
        "\r\n",
        "model.fit(xs, ys, epochs=500) # train model on input data for 500 trials\r\n",
        "\r\n",
        "print(\"Model predicted value for x=10 : {}\" .format ( model.predict([10.0]))) # use model to predict value\r\n",
        "\r\n",
        "# output how model looks like\r\n",
        "print(\"Model parameters: {}\".format(\r\n",
        "                                    first_layer.get_weights()\r\n",
        "                                    ))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjOOe1GwK8hu"
      },
      "source": [
        "# More Complicated Neural Network\r\n",
        "## ML for image classification\r\n",
        "\r\n",
        "\r\n",
        "### Goal \r\n",
        "Train the computer to 'see' twhat type of apparel a given picture shows : e.g. shoe, dress, ankle boot etc) \r\n",
        "\r\n",
        "###Input \r\n",
        "FashionMNIST: DB of B/W images of apparel. \r\n",
        "\r\n",
        "### Data structure : Neural network (NN)\r\n",
        "3 layer NN \r\n",
        "* 0th layer : flattened input layer ( in order to be able to feed values to neurons). \r\n",
        "Layer consist of series of numbers \r\n",
        "2D Image data converted into a 1D series of numbers\r\n",
        "* 1st layer of of 128 neurons\r\n",
        "* 2nd layer of 10 output neurons ( since there are 10 classes in image data set) \r\n",
        "128 is a hyperparameter : it specifies how many neurons are present. It is not model parameter but a parameter that determines the structure of the Neural network \r\n",
        "Each layer defines an Activation function. This defines the functionality of each neuron in that lyaer) \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckVtmEo_o3k2"
      },
      "source": [
        "# hgmodel = keras.Sequential([\r\n",
        "#                          keras.layers.Flatten(input_shape=(28, 28)),    \r\n",
        "#                          keras.layers.Dense(128, activation=tf.nn.relu),    \r\n",
        "#                          keras.layers.Dense(10, activation=tf.nn.softmax)\r\n",
        "#                          ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRPF3EzRq1Ms"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "data = tf.keras.datasets.fashion_mnist\r\n",
        "(training_images, training_labels), (test_images, test_labels) = data.load_data()\r\n",
        "\r\n",
        "training_images  = training_images / 255.0\r\n",
        "test_images = test_images / 255.0\r\n",
        "\r\n",
        "\r\n",
        "model = tf.keras.models.Sequential(\r\n",
        "         [\r\n",
        "           tf.keras.layers.Flatten(input_shape=(28, 28)), # image flattening layer                                    \r\n",
        "           ## the neural network\r\n",
        "           tf.keras.layers.Dense(128, activation=tf.nn.relu),\r\n",
        "           tf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n",
        "         ]\r\n",
        "    )\r\n",
        "\r\n",
        "model.compile(\r\n",
        "    optimizer='adam',\r\n",
        "    loss='sparse_categorical_crossentropy',\r\n",
        "    metrics=['accuracy']\r\n",
        "    )\r\n",
        "\r\n",
        "model.fit(training_images, training_labels, epochs=5)\r\n",
        "\r\n",
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDFAs9QCmrYk"
      },
      "source": [
        "\r\n",
        "# Convolutional Neural Networks\r\n",
        "\r\n",
        "To improve model training turnaround time, the number of parameters in model needs to reduced without losing important parameters information useful to train model. \r\n",
        "\r\n",
        "Convolution and Pooling are two common operations applied in sequence to extract salient features in the model and reduce the input data size while preserving the said features contained in the input data.\r\n",
        "\r\n",
        "##  Convolution \r\n",
        "Convolution is used to emphasize features in am image.\r\n",
        "\r\n",
        "Convolution is implemented as a matrix of coefficients. this matrix is called a filter (a.k.a kernel) . The filter is 'superposed' on top of the image ( which is itself a 28 x 28 matrix) and slid across it while ensuring that the filter matrix does not go out out the image boundary. As the filter matrix is slid across the image, image pixel values are multiplied by filter values and the product is stored into another image matrix. the resultant image matrix is a Convolved version of original image.\r\n",
        "\r\n",
        "In ML Convolution is modelled as a neural network. it consists of neurons. Each neuron performs a  multiplication ( convolution) on each pixel.  The result is fed forward to the output layers where the loss value  ( via loss function) is computed. The error is computed from this loss value and used to compute the scaling factors for filter matrix coefficients. These cofficients are the weights and bias values for that neuron. The cofficients are scaled by the scaling factors and fed back to neuron which computes another convolution value. This process is repeated for each neuron AND repeated as whole until the convolution layer achieves a small loss. \r\n",
        "\r\n",
        "## Pooling\r\n",
        "Pooling is used to reduce image  (input data)  size while keeping features. \r\n",
        "Pooling is typically applied after convolution in order to extract features from convoluted image.  This operation reduces the size of input data and thus improves model performace and accuracy. \r\n",
        "\r\n",
        "In ML, a Pooling layer is used to reduce the number of wieghts and bias values ( effectively reducing the # of neurons). this helps reduce the size of network and thusimprove computation time improving performance. It also helps to enhance accurace of model. lesser nrurons can mean less noise which prents the model from diverging.  \r\n",
        "\r\n",
        "\r\n",
        "# More information: \r\n",
        "1. https://homepages.inf.ed.ac.uk/rbf/HIPR2/convolve.htm\r\n",
        "2. https://blog.francium.tech/machine-learning-convolution-for-image-processing-42623c8dbec0\r\n",
        "\r\n",
        "\r\n",
        "Original NN code: \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0QXeH9q0uFt"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "data = tf.keras.datasets.fashion_mnist\r\n",
        "\r\n",
        "(training_images, training_labels), (test_images, test_labels) = data.load_data()\r\n",
        "\r\n",
        "training_images = training_images / 255.0\r\n",
        "test_images = test_images / 255.0\r\n",
        "\r\n",
        "# the NN definition\r\n",
        "\r\n",
        "# First layer Flatten is only input layer does not have any neurons \r\n",
        "# The 2nd (hidden) and 3rd (output)  layer have 128 and 10 neurons \r\n",
        "# and are densely (Dense)  interconnected.\r\n",
        "\r\n",
        "model = tf.keras.models.Sequential([\r\n",
        "      tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n",
        "      tf.keras.layers.Dense(128, activation=tf.nn.relu),\r\n",
        "      tf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n",
        "    ])\r\n",
        "\r\n",
        "# specify how to train the NN \r\n",
        "model.compile(optimizer='adam',\r\n",
        "       loss='sparse_categorical_crossentropy',\r\n",
        "       metrics=['accuracy'])\r\n",
        "\r\n",
        "# do the training\r\n",
        "model.fit(training_images, training_labels, epochs=5)\r\n",
        "\r\n",
        "model.summary()\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0uL1mDs0vMn"
      },
      "source": [
        "NN Code with Covolution and Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FINZ8Bnis4Ar"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "data = tf.keras.datasets.fashion_mnist\r\n",
        "\r\n",
        "(training_images, training_labels), (test_images, test_labels) = data.load_data()\r\n",
        "\r\n",
        "# massage and normalize training data to be 3D \r\n",
        "training_images = training_images.reshape(60000, 28, 28, 1)\r\n",
        "training_images = training_images / 255.0\r\n",
        "\r\n",
        "# massage and normalize test data to be 3D \r\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\r\n",
        "test_images = test_images / 255.0\r\n",
        "\r\n",
        "model = tf.keras.models.Sequential([\r\n",
        "# convolution and pooling layers                                    \r\n",
        "   tf.keras.layers.Conv2D(64, \r\n",
        "                          (3, 3), \r\n",
        "                          activation='relu', \r\n",
        "                          input_shape=(28, 28, 1)),\r\n",
        "   tf.keras.layers.MaxPooling2D(2, 2),      \r\n",
        "   tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),      \r\n",
        "   tf.keras.layers.MaxPooling2D(2,2),      \r\n",
        "# NN  input layer\r\n",
        "   tf.keras.layers.Flatten(),      \r\n",
        "# NN\r\n",
        "   tf.keras.layers.Dense(128, activation=tf.nn.relu),\r\n",
        "   tf.keras.layers.Dense(10, activation=tf.nn.softmax)    \r\n",
        "   ])\r\n",
        "\r\n",
        "model.compile(optimizer='adam',       \r\n",
        "              loss='sparse_categorical_crossentropy',       \r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(training_images, training_labels, epochs=50)\r\n",
        "\r\n",
        "model.evaluate(test_images, test_labels)\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joNfYLbzsySz"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFAMwm11D5O1"
      },
      "source": [
        "# Complex CNN Example\r\n",
        "\r\n",
        "In this example we will build a CNN that will classify images into one of 2 catehgories : Horsew and Humans.\r\n",
        "\r\n",
        "## Prerequisites\r\n",
        "### Connecting Google Drive to Colab\r\n",
        "Useful to know how we connect our Gdrive to Google Colab so that we can save/load our files from there.  Below link has useful information on this:\r\n",
        "https://www.marktechpost.com/2019/06/07/how-to-connect-google-colab-with-google-drive/\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf6eDyW2G0bv",
        "outputId": "fb890f21-ff3b-4952-9654-2fab3f99392a"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive', force_remount=True);\r\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8-kzyvm-NwB"
      },
      "source": [
        "### Download image data\r\n",
        "Example shows also how to download compressed data from URL and extract it into specific directory.\r\n",
        "\r\n",
        "Download both the treaining and validation data \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHaDR0FTE9v6"
      },
      "source": [
        "import urllib.request\r\n",
        "import zipfile\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\"\r\n",
        "file_name = \"horse-or-human.zip\"\r\n",
        "training_dir = '/content/gdrive/MyDrive/data/horse-or-human/training/'\r\n",
        "urllib.request.urlretrieve(url, file_name)\r\n",
        "zip_ref = zipfile.ZipFile(file_name, 'r')\r\n",
        "zip_ref.extractall(training_dir)\r\n",
        "zip_ref.close()\r\n",
        "\r\n",
        "\r\n",
        "v_url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\"\r\n",
        "validation_file_name = \"validation-horse-or-human.zip\"\r\n",
        "validation_dir = '/content/gdrive/MyDrive/data/horse-or-human/validation/'\r\n",
        "urllib.request.urlretrieve(v_url, validation_file_name)\r\n",
        "zip_ref = zipfile.ZipFile(validation_file_name, 'r')\r\n",
        "zip_ref.extractall(validation_dir)\r\n",
        "zip_ref.close()\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLKhpsH1NXNT"
      },
      "source": [
        "\r\n",
        "### Helpers\r\n",
        "1. Create a model. This will be helpful when we want to restore a saved model\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPc-BP61678E"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "def createModel():\r\n",
        "  \r\n",
        "  model = tf.keras.models.Sequential([\r\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu' , input_shape=(300, 300, 3)),\r\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    tf.keras.layers.Flatten(),\r\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\r\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "  ])\r\n",
        "\r\n",
        "  model.compile(loss='binary_crossentropy',\r\n",
        "        optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\r\n",
        "        metrics=['accuracy'])\r\n",
        "\r\n",
        "  model.summary()\r\n",
        "  return model\r\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlcciHCH9ZjZ"
      },
      "source": [
        "### Create Model input data pipeline \r\n",
        "Automatically assign labels to images\r\n",
        "This also setup a stream \r\n",
        "1. `ImageDataGenerator`  from keras is used to assign labels to images.\r\n",
        "2. `ImageDataGenerator` also sets up input data pipeline to our training/vaidation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cdka-KL9kkd",
        "outputId": "ea43157d-e8f5-4d91-a282-bb066894d62e"
      },
      "source": [
        "# Automatically assign labels to images\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "print (\"Training Data\")\r\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\r\n",
        "training_dir = '/content/gdrive/MyDrive/data/horse-or-human/training/'\r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "  training_dir,\r\n",
        "  target_size=(300, 300),\r\n",
        "  class_mode='binary'\r\n",
        ")\r\n",
        "\r\n",
        "print (\"Validation Data\")\r\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\r\n",
        "validation_dir = '/content/gdrive/MyDrive/data/horse-or-human/validation/'\r\n",
        "validation_generator = validation_datagen.flow_from_directory(\r\n",
        "  validation_dir,\r\n",
        "  target_size=(300, 300),\r\n",
        "  class_mode='binary'\r\n",
        ")\r\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Data\n",
            "Found 1027 images belonging to 2 classes.\n",
            "Validation Data\n",
            "Found 256 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VccAD3b9lly"
      },
      "source": [
        "### Setup Callbacks : Save Model, Stop Training\r\n",
        "1. Save the model every 5 epochs\r\n",
        "#### Saving/Restoring models \r\n",
        "In the code below I have setup logic to save the model data to my gdrive area. In case the training is interrupted I can reload the model and resume from where I left off thus saving valuable time. Below links provide useful information on model saving and restoring:\r\n",
        " 1. https://www.tensorflow.org/guide/keras/save_and_serialize\r\n",
        " 2. https://www.tensorflow.org/tutorials/keras/save_and_load\r\n",
        " 3. SavedModel format https://www.tensorflow.org/guide/saved_model\r\n",
        "\r\n",
        "2. Stop training once desired accuracy is reached"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMXeAeF2NRhw"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import os\r\n",
        "\r\n",
        "########################\r\n",
        "# CallBacks\r\n",
        "#\r\n",
        "# 1. save model after every 5 epochs;  believe default batch size is 32 \r\n",
        "checkpoint_path = \"/content/gdrive/MyDrive/data/hoh_model/training_1/cp-{epoch:04d}.ckpt\"\r\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\r\n",
        "batch_size =32;\r\n",
        "saveTheModel = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\r\n",
        "                                                 save_weights_only=True,\r\n",
        "                                                 save_freq = 5 * batch_size,\r\n",
        "                                                 verbose=1)\r\n",
        "# 2. stop training when desired accuracy is reached\r\n",
        "class stopTraining(tf.keras.callbacks.Callback):\r\n",
        "  def on_epoch_end(self, epoch, logs={}):\r\n",
        "    if(logs.get('accuracy') > 0.999 ):\r\n",
        "      print(\"\\nReached > 98% accuracy so cancelling training!\")\r\n",
        "      \r\n",
        "      self.model.stop_training = True\r\n",
        "\r\n",
        "stopTrainingCBObj = stopTraining();\r\n",
        "##################\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipb8Vo1BEcmw"
      },
      "source": [
        "\r\n",
        "### Setup Model\r\n",
        "1. Generate model topology\r\n",
        "2. setup the check saving format\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDYBr5lCEayD",
        "outputId": "f8dfa3a3-e836-4f18-fecf-fe2d1c3b833d"
      },
      "source": [
        "\r\n",
        "model = createModel();\r\n",
        "model.save_weights(checkpoint_path.format(epoch=0))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_15 (Conv2D)           (None, 298, 298, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 149, 149, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 147, 147, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 73, 73, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 71, 71, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 33, 33, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 1,704,097\n",
            "Trainable params: 1,704,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVHtXLBq_Ten"
      },
      "source": [
        "### Train the model\r\n",
        "Specify the optimization and loss functions and run them for specifed number of epochs \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVe6-Ia3_u8h",
        "outputId": "583c97c5-68ef-4cd5-bb21-3dee80c97f1e"
      },
      "source": [
        "# train the Model\r\n",
        "model_traiiningProfile = model.fit(\r\n",
        "  train_generator,\r\n",
        "  epochs=15,\r\n",
        "  callbacks=[saveTheModel, stopTrainingCBObj],\r\n",
        "  validation_data=validation_generator\r\n",
        ")\r\n",
        "\r\n",
        "# at end of training save in SavedModel format\r\n",
        "model.save('/content/gdrive/MyDrive/data/hoh_saved_model/my_model') \r\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "16/33 [=============>................] - ETA: 39s - loss: 1.3809 - accuracy: 0.5220\n",
            "Epoch 00001: saving model to /content/gdrive/MyDrive/data/hoh_model/training_1/cp-0001.ckpt\n",
            "33/33 [==============================] - 88s 3s/step - loss: 1.1000 - accuracy: 0.5876 - val_loss: 0.8724 - val_accuracy: 0.8320\n",
            "Epoch 2/15\n",
            "33/33 [==============================] - 87s 3s/step - loss: 0.3362 - accuracy: 0.8878 - val_loss: 1.0078 - val_accuracy: 0.8125\n",
            "Epoch 3/15\n",
            "33/33 [==============================] - 87s 3s/step - loss: 0.1573 - accuracy: 0.9465 - val_loss: 1.6067 - val_accuracy: 0.8125\n",
            "Epoch 4/15\n",
            "33/33 [==============================] - 90s 3s/step - loss: 0.1144 - accuracy: 0.9615 - val_loss: 1.4743 - val_accuracy: 0.7461\n",
            "Epoch 5/15\n",
            "33/33 [==============================] - 88s 3s/step - loss: 0.0697 - accuracy: 0.9791 - val_loss: 2.1451 - val_accuracy: 0.8516\n",
            "Epoch 6/15\n",
            "11/33 [=========>....................] - ETA: 50s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 00006: saving model to /content/gdrive/MyDrive/data/hoh_model/training_1/cp-0006.ckpt\n",
            "33/33 [==============================] - 87s 3s/step - loss: 0.0571 - accuracy: 0.9860 - val_loss: 1.5926 - val_accuracy: 0.8477\n",
            "Epoch 7/15\n",
            "33/33 [==============================] - 86s 3s/step - loss: 0.0515 - accuracy: 0.9824 - val_loss: 1.3685 - val_accuracy: 0.8477\n",
            "Epoch 8/15\n",
            "33/33 [==============================] - 86s 3s/step - loss: 0.0118 - accuracy: 0.9984 - val_loss: 3.4694 - val_accuracy: 0.7344\n",
            "Epoch 9/15\n",
            "33/33 [==============================] - 86s 3s/step - loss: 0.0908 - accuracy: 0.9762 - val_loss: 2.6908 - val_accuracy: 0.8516\n",
            "Epoch 10/15\n",
            "33/33 [==============================] - 87s 3s/step - loss: 0.1598 - accuracy: 0.9841 - val_loss: 3.0002 - val_accuracy: 0.8008\n",
            "Epoch 11/15\n",
            " 6/33 [====>.........................] - ETA: 1:07 - loss: 0.0215 - accuracy: 0.9872\n",
            "Epoch 00011: saving model to /content/gdrive/MyDrive/data/hoh_model/training_1/cp-0011.ckpt\n",
            "33/33 [==============================] - 90s 3s/step - loss: 0.0128 - accuracy: 0.9943 - val_loss: 2.8534 - val_accuracy: 0.8203\n",
            "Epoch 12/15\n",
            "33/33 [==============================] - 86s 3s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.9894 - val_accuracy: 0.8477\n",
            "\n",
            "Reached > 98% accuracy so cancelling training!\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/data/hoh_saved_model/my_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNnki3C8FEGj"
      },
      "source": [
        "### Loading a saved model\r\n",
        "The learned weights can saved and reloaded into freshly created model. \r\n",
        "The freshly created model only has the CNN topology, the optimizer setup: loss-func, metrics etc); it does NOT have weight and bias values. \r\n",
        "\r\n",
        "The saved weights/bias can be populated into thusly created model. That model can then be evaluated to identify images without having retrain it.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEebW-777noY",
        "outputId": "9bd74d4a-602c-465a-fb5f-d06967aba301"
      },
      "source": [
        "checkpoint_path = \"/content/gdrive/MyDrive/data/hoh_model/training_1/cp-{epoch:04d}.ckpt\"\r\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\r\n",
        "print (\"Checkpoint directory : \" +  checkpoint_dir)\r\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\r\n",
        "print (\"Latest checkpoint \" +  latest)\r\n",
        "# Create a new model instance\r\n",
        "model = createModel()\r\n",
        "\r\n",
        "\r\n",
        "# evaluate without loading weights ( untrained model)\r\n",
        "loss, acc = model.evaluate(train_generator)\r\n",
        "print(\"Untrained model on Training data, accuracy: {:5.2f}%\".format(100 * acc))\r\n",
        "loss, acc = model.evaluate(validation_generator)\r\n",
        "print(\"Untrained Model on validation data, accuracy: {:5.2f}%\".format(100 * acc))\r\n",
        "# Load the previously saved weights\r\n",
        "# evaluate after loading weights ( trained model) \r\n",
        "model.load_weights(latest)\r\n",
        "loss, acc = model.evaluate(train_generator)\r\n",
        "print(\"Trained model on Training data, accuracy: {:5.2f}%\".format(100 * acc))\r\n",
        "loss, acc = model.evaluate(validation_generator)\r\n",
        "print(\"Trainedmodel on validation data, accuracy: {:5.2f}%\".format(100 * acc))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint directory : /content/gdrive/MyDrive/data/hoh_model/training_1\n",
            "Latest checkpoint /content/gdrive/MyDrive/data/hoh_model/training_1/cp-0011.ckpt\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_55 (Conv2D)           (None, 298, 298, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_55 (MaxPooling (None, 149, 149, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_56 (Conv2D)           (None, 147, 147, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_56 (MaxPooling (None, 73, 73, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_57 (Conv2D)           (None, 71, 71, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_57 (MaxPooling (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_58 (Conv2D)           (None, 33, 33, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_58 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_59 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_59 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 1,704,097\n",
            "Trainable params: 1,704,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "33/33 [==============================] - 50s 1s/step - loss: 0.6912 - accuracy: 0.5406\n",
            "Untrained model on Training data, accuracy: 53.94%\n",
            "8/8 [==============================] - 6s 765ms/step - loss: 0.6908 - accuracy: 0.5117\n",
            "Untrained Model on validation data, accuracy: 51.17%\n",
            "33/33 [==============================] - 49s 1s/step - loss: 1.4563 - accuracy: 0.7741\n",
            "Trained model on Training data, accuracy: 77.41%\n",
            "8/8 [==============================] - 6s 764ms/step - loss: 3.4493 - accuracy: 0.7969\n",
            "Trainedmodel on validation data, accuracy: 79.69%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTDWzq7fHXCi"
      },
      "source": [
        "### Testing model\r\n",
        "\r\n",
        "Code Taken from:\r\n",
        "\r\n",
        "https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Horse_or_Human_NoValidation.ipynb#scrollTo=DoWp43WxJDNT\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "TLhSwAUJ-GFP",
        "outputId": "737f7b0b-6bbd-400e-c010-f3025a6b7dd1"
      },
      "source": [
        "\r\n",
        "import numpy as np\r\n",
        "from google.colab import files\r\n",
        "from keras.preprocessing import image\r\n",
        "\r\n",
        "uploaded = files.upload()\r\n",
        "\r\n",
        "for fn in uploaded.keys():\r\n",
        " \r\n",
        "  # predicting images\r\n",
        "  path = '/content/' + fn\r\n",
        "  img = image.load_img(path, target_size=(300, 300))\r\n",
        "  x = image.img_to_array(img)\r\n",
        "  x = np.expand_dims(x, axis=0)\r\n",
        "\r\n",
        "  images = np.vstack([x])\r\n",
        "  classes = model.predict(images, batch_size=10)\r\n",
        "  print(classes[0])\r\n",
        "  if classes[0]>0.5:\r\n",
        "    print(fn + \" is a human\")\r\n",
        "  else:\r\n",
        "    print(fn + \" is a horse\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0248faba-29d5-44e9-a82f-9ade108c0913\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0248faba-29d5-44e9-a82f-9ade108c0913\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving horse1.jpg to horse1.jpg\n",
            "Saving horse2.jpg to horse2.jpg\n",
            "Saving human1.jpg to human1.jpg\n",
            "Saving human1PNG..png to human1PNG..png\n",
            "[1.]\n",
            "horse1.jpg is a human\n",
            "[1.]\n",
            "horse2.jpg is a human\n",
            "[1.]\n",
            "human1.jpg is a human\n",
            "[1.]\n",
            "human1PNG..png is a human\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8vg8-FcN4Ro"
      },
      "source": [
        "### Effect of convolution and pooling\r\n",
        "studying the convolution and pooling layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boupy0YzOv-z",
        "outputId": "44b0d77f-66cd-4a4e-ad7b-1ac7e06fe009"
      },
      "source": [
        "!ls /content"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive\t    horse2.jpg\t\thuman1PNG..png\n",
            "gdrive\t    horse-or-human.zip\tsample_data\n",
            "horse1.jpg  human1.jpg\t\tvalidation-horse-or-human.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44VeV31eN4_U"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\r\n",
        "\r\n",
        "# Let's define a new Model that will take an image as input, and will output\r\n",
        "# intermediate representations for all layers in the previous model after\r\n",
        "# the first.\r\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\r\n",
        "#visualization_model = Model(img_input, successive_outputs)\r\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\r\n",
        "\r\n",
        "\r\n",
        "# DMK # Let's prepare a random input image from the training set.\r\n",
        "# DMK horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\r\n",
        "# DMK human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\r\n",
        "# DMK img_path = random.choice(horse_img_files + human_img_files)\r\n",
        "img_path = '/content/horse1.jpg'\r\n",
        "\r\n",
        "img = load_img(img_path, target_size=(300, 300))  # this is a PIL image\r\n",
        "x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\r\n",
        "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\r\n",
        "\r\n",
        "# Rescale by 1/255\r\n",
        "x /= 255\r\n",
        "\r\n",
        "# Let's run our image through our network, thus obtaining all\r\n",
        "# intermediate representations for this image.\r\n",
        "successive_feature_maps = visualization_model.predict(x)\r\n",
        "\r\n",
        "# These are the names of the layers, so can have them as part of our plot\r\n",
        "layer_names = [layer.name for layer in model.layers]\r\n",
        "\r\n",
        "# Now let's display our representations\r\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\r\n",
        "  if len(feature_map.shape) == 4:\r\n",
        "    \r\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\r\n",
        "    n_features = feature_map.shape[-1]  # number of features in feature map\r\n",
        "    \r\n",
        "    # The feature map has shape (1, size, size, n_features)\r\n",
        "    size = feature_map.shape[1]\r\n",
        "    # We will tile our images in this matrix\r\n",
        "    display_grid = np.zeros((size, size * n_features))\r\n",
        "    for i in range(n_features):\r\n",
        "      # Postprocess the feature to make it visually palatable\r\n",
        "      x = feature_map[0, :, :, i]\r\n",
        "      x -= x.mean()\r\n",
        "      x /= x.std()\r\n",
        "      x *= 64\r\n",
        "      x += 128\r\n",
        "      x = np.clip(x, 0, 255).astype('uint8')\r\n",
        "      # We'll tile each filter into this big horizontal grid\r\n",
        "      display_grid[:, i * size : (i + 1) * size] = x\r\n",
        "    # Display the grid\r\n",
        "    scale = 20. / n_features\r\n",
        "    plt.figure(figsize=(scale * n_features, scale))\r\n",
        "    plt.title(layer_name)\r\n",
        "    plt.grid(False)\r\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr2C-8PreOHq"
      },
      "source": [
        "# Augmentation\r\n",
        "\r\n",
        "## Create Model and train with augmented data\r\n",
        "This is a technique to generate additional variations from the training dataset. These variations  + training dataset are then fed into training process to further refine the quality of the CNN model\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hof9k852fIcC",
        "outputId": "77671b5c-eccb-4d07-d1d7-925cc4a0b8b9"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "########################\r\n",
        "#\r\n",
        "# Generate labels to images\r\n",
        "# Augment the input training data with variations\r\n",
        "# Setup the training generator to regeneerate the new training set \r\n",
        "#\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "  rescale=1./255,\r\n",
        "  rotation_range=40,\r\n",
        "  width_shift_range=0.2,\r\n",
        "  height_shift_range=0.2,\r\n",
        "  shear_range=0.2,\r\n",
        "  zoom_range=0.2,\r\n",
        "  horizontal_flip=True,\r\n",
        "  fill_mode='nearest'\r\n",
        ")\r\n",
        "training_dir = '/content/gdrive/MyDrive/data/horse-or-human/training/'\r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "  training_dir,\r\n",
        "  target_size=(300, 300),\r\n",
        "  class_mode='binary'\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "########################\r\n",
        "#\r\n",
        "# Create a new model instance\r\n",
        "#\r\n",
        "model = createModel()\r\n",
        "\r\n",
        "\r\n",
        "########################\r\n",
        "#\r\n",
        "# CallBacks\r\n",
        "#\r\n",
        "# 1. save model after every 5 epochs;  believe default batch size is 32 \r\n",
        "checkpoint_path = \"/content/gdrive/MyDrive/data/hoh_model/aug_training_1/cp-{epoch:04d}.ckpt\"\r\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\r\n",
        "batch_size =32;\r\n",
        "saveTheModel = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\r\n",
        "                                                 save_weights_only=True,\r\n",
        "                                                 save_freq = 5 * batch_size,\r\n",
        "                                                 verbose=1)\r\n",
        "# 2. stop training when desired accuracy is reached\r\n",
        "class stopTraining(tf.keras.callbacks.Callback):\r\n",
        "  def on_epoch_end(self, epoch, logs={}):\r\n",
        "    if(logs.get('accuracy')>0.999):\r\n",
        "      print(\"\\nReached > 98% accuracy so cancelling training!\")\r\n",
        "      self.model.stop_training = True\r\n",
        "\r\n",
        "stopTrainingCBObj = stopTraining();\r\n",
        "\r\n",
        "\r\n",
        "####################\r\n",
        "#\r\n",
        "# train model\r\n",
        "#\r\n",
        "featureAugTrainingProfile = model.fit(\r\n",
        "  train_generator,\r\n",
        "  epochs=45,\r\n",
        "  callbacks=[saveTheModel, stopTrainingCBObj]\r\n",
        ")\r\n",
        "\r\n",
        "# at end of training save in SavedModel format\r\n",
        "model.save('/content/gdrive/MyDrive/data/hoh_saved_aug_model/my_model') \r\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_45 (Conv2D)           (None, 298, 298, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_45 (MaxPooling (None, 149, 149, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 147, 147, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_46 (MaxPooling (None, 73, 73, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 71, 71, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_47 (MaxPooling (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 33, 33, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_48 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_49 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 1,704,097\n",
            "Trainable params: 1,704,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/45\n",
            "33/33 [==============================] - 101s 3s/step - loss: 0.8999 - accuracy: 0.5588\n",
            "Epoch 2/45\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.5516 - accuracy: 0.7251\n",
            "Epoch 3/45\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.5082 - accuracy: 0.7569\n",
            "Epoch 4/45\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.5433 - accuracy: 0.7511\n",
            "Epoch 5/45\n",
            "28/33 [========================>.....] - ETA: 14s - loss: 0.5466 - accuracy: 0.7674\n",
            "Epoch 00005: saving model to /content/gdrive/MyDrive/data/hoh_model/aug_training_1/cp-0005.ckpt\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.5235 - accuracy: 0.7746\n",
            "Epoch 6/45\n",
            "33/33 [==============================] - 103s 3s/step - loss: 0.3362 - accuracy: 0.8490\n",
            "Epoch 7/45\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.2454 - accuracy: 0.9058\n",
            "Epoch 8/45\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.3550 - accuracy: 0.8504\n",
            "Epoch 9/45\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.3818 - accuracy: 0.8858\n",
            "Epoch 10/45\n",
            "23/33 [===================>..........] - ETA: 29s - loss: 0.1721 - accuracy: 0.9359\n",
            "Epoch 00010: saving model to /content/gdrive/MyDrive/data/hoh_model/aug_training_1/cp-0010.ckpt\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.1690 - accuracy: 0.9361\n",
            "Epoch 11/45\n",
            "33/33 [==============================] - 99s 3s/step - loss: 0.3015 - accuracy: 0.8977\n",
            "Epoch 12/45\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.2537 - accuracy: 0.8874\n",
            "Epoch 13/45\n",
            "33/33 [==============================] - 102s 3s/step - loss: 0.1457 - accuracy: 0.9521\n",
            "Epoch 14/45\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.1370 - accuracy: 0.9675\n",
            "Epoch 15/45\n",
            "18/33 [===============>..............] - ETA: 46s - loss: 0.2154 - accuracy: 0.9328\n",
            "Epoch 00015: saving model to /content/gdrive/MyDrive/data/hoh_model/aug_training_1/cp-0015.ckpt\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.2279 - accuracy: 0.9368\n",
            "Epoch 16/45\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.0823 - accuracy: 0.9697\n",
            "Epoch 17/45\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.1274 - accuracy: 0.9493\n",
            "Epoch 18/45\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.1749 - accuracy: 0.9656\n",
            "Epoch 19/45\n",
            "33/33 [==============================] - 103s 3s/step - loss: 0.1254 - accuracy: 0.9576\n",
            "Epoch 20/45\n",
            "13/33 [==========>...................] - ETA: 57s - loss: 0.0402 - accuracy: 0.9890\n",
            "Epoch 00020: saving model to /content/gdrive/MyDrive/data/hoh_model/aug_training_1/cp-0020.ckpt\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.0634 - accuracy: 0.9789\n",
            "Epoch 21/45\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.0886 - accuracy: 0.9688\n",
            "Epoch 22/45\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.1074 - accuracy: 0.9598\n",
            "Epoch 23/45\n",
            "33/33 [==============================] - 99s 3s/step - loss: 0.0909 - accuracy: 0.9734\n",
            "Epoch 24/45\n",
            "33/33 [==============================] - 99s 3s/step - loss: 0.2031 - accuracy: 0.9490\n",
            "Epoch 25/45\n",
            " 8/33 [======>.......................] - ETA: 1:29 - loss: 0.1889 - accuracy: 0.9251\n",
            "Epoch 00025: saving model to /content/gdrive/MyDrive/data/hoh_model/aug_training_1/cp-0025.ckpt\n",
            "33/33 [==============================] - 103s 3s/step - loss: 0.1314 - accuracy: 0.9492\n",
            "Epoch 26/45\n",
            "33/33 [==============================] - 99s 3s/step - loss: 0.0531 - accuracy: 0.9863\n",
            "Epoch 27/45\n",
            "33/33 [==============================] - 99s 3s/step - loss: 0.1146 - accuracy: 0.9621\n",
            "Epoch 28/45\n",
            "33/33 [==============================] - 99s 3s/step - loss: 0.0736 - accuracy: 0.9808\n",
            "Epoch 29/45\n",
            "33/33 [==============================] - 99s 3s/step - loss: 0.0472 - accuracy: 0.9831\n",
            "Epoch 30/45\n",
            " 3/33 [=>............................] - ETA: 1:31 - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 00030: saving model to /content/gdrive/MyDrive/data/hoh_model/aug_training_1/cp-0030.ckpt\n",
            "33/33 [==============================] - 99s 3s/step - loss: 0.0379 - accuracy: 0.9890\n",
            "Epoch 31/45\n",
            "33/33 [==============================] - 103s 3s/step - loss: 0.0544 - accuracy: 0.9840\n",
            "Epoch 32/45\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.0280 - accuracy: 0.9917\n",
            "Epoch 33/45\n",
            "33/33 [==============================] - 99s 3s/step - loss: 0.0707 - accuracy: 0.9829\n",
            "Epoch 34/45\n",
            "31/33 [===========================>..] - ETA: 5s - loss: 0.0466 - accuracy: 0.9836\n",
            "Epoch 00034: saving model to /content/gdrive/MyDrive/data/hoh_model/aug_training_1/cp-0034.ckpt\n",
            "33/33 [==============================] - 99s 3s/step - loss: 0.0459 - accuracy: 0.9836\n",
            "Epoch 35/45\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.0543 - accuracy: 0.9843\n",
            "Epoch 36/45\n",
            "33/33 [==============================] - 99s 3s/step - loss: 0.0566 - accuracy: 0.9808\n",
            "Epoch 37/45\n",
            "33/33 [==============================] - 103s 3s/step - loss: 0.0577 - accuracy: 0.9851\n",
            "Epoch 38/45\n",
            "33/33 [==============================] - 99s 3s/step - loss: 0.0528 - accuracy: 0.9807\n",
            "Epoch 39/45\n",
            "26/33 [======================>.......] - ETA: 20s - loss: 0.0328 - accuracy: 0.9882\n",
            "Epoch 00039: saving model to /content/gdrive/MyDrive/data/hoh_model/aug_training_1/cp-0039.ckpt\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.0368 - accuracy: 0.9872\n",
            "Epoch 40/45\n",
            "33/33 [==============================] - 99s 3s/step - loss: 0.0412 - accuracy: 0.9860\n",
            "Epoch 41/45\n",
            "33/33 [==============================] - 99s 3s/step - loss: 0.0359 - accuracy: 0.9908\n",
            "Epoch 42/45\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.0802 - accuracy: 0.9814\n",
            "Epoch 43/45\n",
            "33/33 [==============================] - 104s 3s/step - loss: 0.0936 - accuracy: 0.9733\n",
            "Epoch 44/45\n",
            "21/33 [==================>...........] - ETA: 37s - loss: 0.7256 - accuracy: 0.9073\n",
            "Epoch 00044: saving model to /content/gdrive/MyDrive/data/hoh_model/aug_training_1/cp-0044.ckpt\n",
            "33/33 [==============================] - 100s 3s/step - loss: 0.5413 - accuracy: 0.9312\n",
            "Epoch 45/45\n",
            "33/33 [==============================] - 99s 3s/step - loss: 0.0364 - accuracy: 0.9847\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/data/hoh_saved_aug_model/my_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6vhPzIvRti7"
      },
      "source": [
        "## Check Augumented model \r\n",
        "Reload the saved auugmented model and evaluate its 'efficacy' with validation data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lUV1XYaRzb3",
        "outputId": "cd223e76-fd1d-4d04-be4a-26d2d6b5e5d0"
      },
      "source": [
        "\r\n",
        "checkpoint_path = \"/content/gdrive/MyDrive/data/hoh_model/aug_training_1/cp-{epoch:04d}.ckpt\"\r\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\r\n",
        "print (\"Checkpoint directory : \" +  checkpoint_dir)\r\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\r\n",
        "print (\"Latest checkpoint \" +  latest)\r\n",
        "\r\n",
        "# Create a new model instance\r\n",
        "model = createModel()\r\n",
        "\r\n",
        "# evaluate without loading weights ( untrained model)\r\n",
        "loss, acc = model.evaluate(train_generator)\r\n",
        "print(\"Untrained model on Training data, accuracy: {:5.2f}%\".format(100 * acc))\r\n",
        "loss, acc = model.evaluate(validation_generator)\r\n",
        "print(\"Untrained Model on validation data, accuracy: {:5.2f}%\".format(100 * acc))\r\n",
        "# Load the previously saved weights\r\n",
        "# evaluate after loading weights ( trained model) \r\n",
        "model.load_weights(latest)\r\n",
        "loss, acc = model.evaluate(train_generator)\r\n",
        "print(\"Trained model on Training data, accuracy: {:5.2f}%\".format(100 * acc))\r\n",
        "loss, acc = model.evaluate(validation_generator)\r\n",
        "print(\"Trainedmodel on validation data, accuracy: {:5.2f}%\".format(100 * acc))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint directory : /content/gdrive/MyDrive/data/hoh_model/aug_training_1\n",
            "Latest checkpoint /content/gdrive/MyDrive/data/hoh_model/aug_training_1/cp-0044.ckpt\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_60 (Conv2D)           (None, 298, 298, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_60 (MaxPooling (None, 149, 149, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_61 (Conv2D)           (None, 147, 147, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_61 (MaxPooling (None, 73, 73, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_62 (Conv2D)           (None, 71, 71, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_62 (MaxPooling (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_63 (Conv2D)           (None, 33, 33, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_63 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_64 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_64 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 1,704,097\n",
            "Trainable params: 1,704,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "33/33 [==============================] - 50s 1s/step - loss: 0.6968 - accuracy: 0.4747\n",
            "Untrained model on Training data, accuracy: 48.49%\n",
            "8/8 [==============================] - 6s 769ms/step - loss: 0.6872 - accuracy: 0.5000\n",
            "Untrained Model on validation data, accuracy: 50.00%\n",
            "33/33 [==============================] - 52s 2s/step - loss: 0.0233 - accuracy: 0.9922\n",
            "Trained model on Training data, accuracy: 99.22%\n",
            "8/8 [==============================] - 7s 765ms/step - loss: 2.5842 - accuracy: 0.8164\n",
            "Trainedmodel on validation data, accuracy: 81.64%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCpM4j7R1l-Y"
      },
      "source": [
        "# Transfer Learning\r\n",
        "\r\n",
        "We can take a pre-trained model and attach it our model to improve the accuracy of our model. The pretrained model can another model trained to recognize more images than ours. We just 'connect' that model to our model and run our training.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0Rny6LK1ljr",
        "outputId": "1ad9b8b1-d67a-488f-aa8a-f9bc14563b20"
      },
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\r\n",
        "\r\n",
        "weights_url = \"https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\r\n",
        "\r\n",
        "weights_file = \"/content/gdrive/MyDrive/data/inceptionv3/inception_v3.h5\"\r\n",
        "urllib.request.urlretrieve(weights_url, weights_file)\r\n",
        "\r\n",
        "pre_trained_model = InceptionV3(input_shape=(300, 300, 3),\r\n",
        "                include_top=False,\r\n",
        "                weights=None)\r\n",
        "\r\n",
        "pre_trained_model.load_weights(weights_file)\r\n",
        "\r\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.rho\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzUH1j5CsdbG"
      },
      "source": [
        "### Inception model structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxVlfutwscmQ"
      },
      "source": [
        "pre_trained_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3W1qSEstcnE"
      },
      "source": [
        "### Connecting the Inception Model to our model\r\n",
        "\r\n",
        "We take the inception model and select one of layers to connect to our model.\r\n",
        "\r\n",
        "Before training, we lock the Inception model layers from being trained by our optimizer. \r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sirR6z-stu3N",
        "outputId": "45731c52-ec66-49b4-e24d-6b42696dfdfa"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "for layer in pre_trained_model.layers:\r\n",
        "  layer.trainable = False\r\n",
        "\r\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\r\n",
        "print('last layer output shape: ', last_layer.output_shape)\r\n",
        "last_output = last_layer.output\r\n",
        "\r\n",
        "# Flatten the output layer to 1 dimension\r\n",
        "x = tf.keras.layers.Flatten()(last_output)\r\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\r\n",
        "x = tf.keras.layers.Dense(1024, activation='relu')(x)\r\n",
        "# Add a final sigmoid layer for classification\r\n",
        "x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\r\n",
        "\r\n",
        "model = tf.keras.Model(pre_trained_model.input, x)\r\n",
        "\r\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\r\n",
        "       loss='binary_crossentropy',\r\n",
        "       metrics=['acc'])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "last layer output shape:  (None, 17, 17, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zpMLpJQvgGR",
        "outputId": "adb5c832-79c2-458a-eea0-a66c771bdd48"
      },
      "source": [
        "checkpoint_path = \"/content/gdrive/MyDrive/data/hoh_model/tl_training_1/cp-{epoch:04d}.ckpt\"\r\n",
        "\r\n",
        "tl_TrainingProfile = model.fit(\r\n",
        "  train_generator,\r\n",
        "  epochs=45,\r\n",
        "  callbacks=[saveTheModel, stopTrainingCBObj],\r\n",
        "  validation_data=validation_generator\r\n",
        ")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/45\n",
            "10/33 [========>.....................] - ETA: 2:20 - loss: 3.3397 - acc: 0.6131"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}